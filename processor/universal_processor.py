#!/usr/bin/env python3
import sys
import time
import re
from datetime import datetime
from threading import Timer
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient, models

class UniversalLogProcessor:
    def __init__(self, collection_name="universal-logs"):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.client = QdrantClient("localhost")
        self.collection_name = collection_name
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        self.init_collection()
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –±–∞—Ç—á–∏–Ω–≥–∞
        self.batch_size = 15
        self.batch_timeout = 3  # —Å–µ–∫—É–Ω–¥—ã
        self.batch_buffer = []
        self.flush_timer = None
        self.reset_timer()
        
        # –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        self.processed_count = 0
        self.start_time = time.time()
    
    def init_collection(self):
        """–°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"""
        try:
            self.client.get_collection(self.collection_name)
        except Exception:
            # –ö–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=models.VectorParams(
                    size=384,  # all-MiniLM-L6-v2 dimension
                    distance=models.Distance.COSINE
                )
            )
            print(f"‚úÖ Created collection: {self.collection_name}")
    
    def reset_timer(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ç–∞–π–º–µ—Ä –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –æ—Ç–ø—Ä–∞–≤–∫–∏ –±–∞—Ç—á–∞"""
        if self.flush_timer:
            self.flush_timer.cancel()
        self.flush_timer = Timer(self.batch_timeout, self.flush_batch)
        self.flush_timer.start()
    
    def extract_log_metadata(self, line):
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ —Å—Ç—Ä–æ–∫–∏ –ª–æ–≥–∞"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ª–æ–≥–æ–≤
        patterns = [
            # JSON –ª–æ–≥–∏: {"timestamp": "...", "level": "ERROR", "message": "..."}
            (r'^{.*"level"\s*:\s*"(\w+)".*"message"\s*:\s*"([^"]+)".*}$', self.parse_json_log),
            
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ª–æ–≥–∏: [INFO] 2024-01-15 message
            (r'^\[(\w+)\]\s+(\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2})\s+(.+)$', self.parse_bracket_log),
            
            # Nginx/Apache: 192.168.1.1 - - [15/Jan/2024:10:30:00] "GET /"
            (r'^(\d+\.\d+\.\d+\.\d+).*\[(.+?)\].*"(GET|POST|PUT|DELETE)', self.parse_web_log),
        ]
        
        for pattern, parser in patterns:
            match = re.match(pattern, line.strip())
            if match:
                return parser(match, line)
        
        # Fallback: –ø—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç
        return {
            "message": line.strip(),
            "level": self.detect_log_level(line),
            "timestamp": datetime.now().isoformat(),
            "source": "stdin",
            "format": "plain"
        }
    
    def parse_json_log(self, match, original_line):
        """–ü–∞—Ä—Å–∏–º JSON –ª–æ–≥–∏"""
        try:
            data = json.loads(original_line)
            return {
                "message": data.get("message", original_line),
                "level": data.get("level", "INFO"),
                "timestamp": data.get("timestamp", datetime.now().isoformat()),
                "source": data.get("source", "unknown"),
                "format": "json"
            }
        except:
            return self.fallback_parse(original_line)
    
    def parse_bracket_log(self, match, original_line):
        """–ü–∞—Ä—Å–∏–º –ª–æ–≥–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [LEVEL] timestamp message"""
        return {
            "message": match.group(3),
            "level": match.group(1),
            "timestamp": match.group(2),
            "source": "application", 
            "format": "bracketed"
        }
    
    def parse_web_log(self, match, original_line):
        """–ü–∞—Ä—Å–∏–º –≤–µ–±-–ª–æ–≥–∏"""
        return {
            "message": original_line.strip(),
            "level": "INFO",  # web logs –æ–±—ã—á–Ω–æ INFO
            "timestamp": match.group(2),
            "source": "web_server",
            "client_ip": match.group(1),
            "format": "web"
        }
    
    def fallback_parse(self, line):
        """Fallback –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤"""
        return {
            "message": line.strip(),
            "level": self.detect_log_level(line),
            "timestamp": datetime.now().isoformat(), 
            "source": "unknown",
            "format": "plain"
        }
    
    def detect_log_level(self, message):
        """–ê–≤—Ç–æ-–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è –ª–æ–≥–∞"""
        message_lower = message.lower()
        
        error_keywords = ['error', 'exception', 'failed', 'fatal', 'crash', 'panic']
        warn_keywords = ['warn', 'warning', 'deprecated', 'slow', 'timeout']
        debug_keywords = ['debug', 'trace', 'verbose']
        
        if any(word in message_lower for word in error_keywords):
            return "ERROR"
        elif any(word in message_lower for word in warn_keywords):
            return "WARN" 
        elif any(word in message_lower for word in debug_keywords):
            return "DEBUG"
        else:
            return "INFO"
    
    def process_line(self, line):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –∏–∑ stdin"""
        if not line.strip():
            return
            
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            log_data = self.extract_log_metadata(line)
            self.batch_buffer.append(log_data)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–∞–π–º–µ—Ä
            self.reset_timer()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            if len(self.batch_buffer) >= self.batch_size:
                self.flush_batch()
                
            self.processed_count += 1
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π —Å—Ç–∞—Ç—É—Å
            if self.processed_count % 100 == 0:
                elapsed = time.time() - self.start_time
                rate = self.processed_count / elapsed
                print(f"üìä Processed {self.processed_count} logs ({rate:.1f}/sec)", 
                      file=sys.stderr)
                      
        except Exception as e:
            print(f"‚ùå Error processing line: {e}", file=sys.stderr)
    
    def flush_batch(self):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–≥–æ –±–∞—Ç—á–∞ –≤ Qdrant"""
        if not self.batch_buffer:
            return
            
        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –±–∞—Ç—á–µ
            messages = [log["message"] for log in self.batch_buffer]
            embeddings = self.model.encode(messages)
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ—á–∫–∏ –¥–ª—è Qdrant
            points = []
            for i, (log, embedding) in enumerate(zip(self.batch_buffer, embeddings)):
                point_id = int(time.time() * 1000000) + i  # microsecond precision
                
                points.append(models.PointStruct(
                    id=point_id,
                    vector=embedding.tolist(),
                    payload={
                        "message": log["message"],
                        "level": log["level"],
                        "timestamp": log["timestamp"],
                        "source": log["source"],
                        "format": log["format"],
                        "processed_at": datetime.now().isoformat(),
                        "batch_size": len(self.batch_buffer)
                    }
                ))
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Qdrant
            self.client.upsert(
                collection_name=self.collection_name,
                points=points
            )
            
            print(f"‚úÖ Saved {len(points)} logs to {self.collection_name}", 
                  file=sys.stderr)
            
        except Exception as e:
            print(f"‚ùå Batch flush error: {e}", file=sys.stderr)
        finally:
            self.batch_buffer.clear()
    
    def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ stdin"""
        print(f"üöÄ Universal Log Processor started", file=sys.stderr)
        print(f"üìÅ Collection: {self.collection_name}", file=sys.stderr)
        print(f"‚öôÔ∏è  Batch: {self.batch_size} logs or {self.batch_timeout}s", file=sys.stderr)
        print("---", file=sys.stderr)
        
        try:
            for line in sys.stdin:
                self.process_line(line)
                
        except KeyboardInterrupt:
            print("\nüõë Shutdown signal received...", file=sys.stderr)
        except BrokenPipeError:
            print("üí• Input pipe broken", file=sys.stderr)
        except Exception as e:
            print(f"üí• Fatal error: {e}", file=sys.stderr)
        finally:
            # –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ª–æ–≥–∏
            if self.flush_timer:
                self.flush_timer.cancel()
            if self.batch_buffer:
                print(f"üíæ Flushing {len(self.batch_buffer)} remaining logs...", 
                      file=sys.stderr)
                self.flush_batch()
            
            elapsed = time.time() - self.start_time
            print(f"üëã Processor stopped. Stats:", file=sys.stderr)
            print(f"   Total processed: {self.processed_count} logs", file=sys.stderr)
            print(f"   Duration: {elapsed:.1f} seconds", file=sys.stderr)
            print(f"   Rate: {self.processed_count/elapsed:.1f} logs/sec", file=sys.stderr)

if __name__ == "__main__":
    # –ú–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –∏–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ —á–µ—Ä–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç
    collection_name = sys.argv[1] if len(sys.argv) > 1 else "universal-logs"
    processor = UniversalLogProcessor(collection_name)
    processor.run()
