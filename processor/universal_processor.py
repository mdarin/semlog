#!/usr/bin/env python3
import sys
import time
import re
from datetime import datetime
from threading import Timer
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient, models

class UniversalLogProcessor:
    def __init__(self, collection_name="universal-logs"):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.client = QdrantClient("localhost")
        self.collection_name = collection_name
        
        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸ÑŽ ÐµÑÐ»Ð¸ ÐµÑ‘ Ð½ÐµÑ‚
        self.init_collection()
        
        # ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð±Ð°Ñ‚Ñ‡Ð¸Ð½Ð³Ð°
        self.batch_size = 15
        self.batch_timeout = 3  # ÑÐµÐºÑƒÐ½Ð´Ñ‹
        self.batch_buffer = []
        self.flush_timer = None
        self.reset_timer()
        
        # Ð¡Ñ‡ÐµÑ‚Ñ‡Ð¸ÐºÐ¸ Ð´Ð»Ñ Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³Ð°
        self.processed_count = 0
        self.start_time = time.time()
    
    def init_collection(self):
        """Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸ÑŽ ÐµÑÐ»Ð¸ Ð½Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚"""
        try:
            self.client.get_collection(self.collection_name)
        except Exception:
            # ÐšÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ Ð½Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚, ÑÐ¾Ð·Ð´Ð°ÐµÐ¼
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=models.VectorParams(
                    size=384,  # all-MiniLM-L6-v2 dimension
                    distance=models.Distance.COSINE
                )
            )
            print(f"âœ… Created collection: {self.collection_name}")
    
    def reset_timer(self):
        """Ð¡Ð±Ñ€Ð°ÑÑ‹Ð²Ð°ÐµÐ¼ Ñ‚Ð°Ð¹Ð¼ÐµÑ€ Ð´Ð»Ñ Ð¿Ñ€Ð¸Ð½ÑƒÐ´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐ¸ Ð±Ð°Ñ‚Ñ‡Ð°"""
        if self.flush_timer:
            self.flush_timer.cancel()
        self.flush_timer = Timer(self.batch_timeout, self.flush_batch)
        self.flush_timer.start()
    
    def extract_log_metadata(self, line):
        """Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð»Ð¾Ð³Ð°"""
        # ÐŸÐ°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ Ð´Ð»Ñ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¾Ð² Ð»Ð¾Ð³Ð¾Ð²
        patterns = [
            # JSON Ð»Ð¾Ð³Ð¸: {"timestamp": "...", "level": "ERROR", "message": "..."}
            (r'^{.*"level"\s*:\s*"(\w+)".*"message"\s*:\s*"([^"]+)".*}$', self.parse_json_log),
            
            # Ð¡Ñ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ðµ Ð»Ð¾Ð³Ð¸: [INFO] 2024-01-15 message
            (r'^\[(\w+)\]\s+(\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2})\s+(.+)$', self.parse_bracket_log),
            
            # Nginx/Apache: 192.168.1.1 - - [15/Jan/2024:10:30:00] "GET /"
            (r'^(\d+\.\d+\.\d+\.\d+).*\[(.+?)\].*"(GET|POST|PUT|DELETE)', self.parse_web_log),
        ]
        
        for pattern, parser in patterns:
            match = re.match(pattern, line.strip())
            if match:
                return parser(match, line)
        
        # Fallback: Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ Ñ‚ÐµÐºÑÑ‚
        return {
            "message": line.strip(),
            "level": self.detect_log_level(line),
            "timestamp": datetime.now().isoformat(),
            "source": "stdin",
            "format": "plain"
        }
    
    def parse_json_log(self, match, original_line):
        """ÐŸÐ°Ñ€ÑÐ¸Ð¼ JSON Ð»Ð¾Ð³Ð¸"""
        try:
            data = json.loads(original_line)
            return {
                "message": data.get("message", original_line),
                "level": data.get("level", "INFO"),
                "timestamp": data.get("timestamp", datetime.now().isoformat()),
                "source": data.get("source", "unknown"),
                "format": "json"
            }
        except:
            return self.fallback_parse(original_line)
    
    def parse_bracket_log(self, match, original_line):
        """ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ð»Ð¾Ð³Ð¸ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ [LEVEL] timestamp message"""
        return {
            "message": match.group(3),
            "level": match.group(1),
            "timestamp": match.group(2),
            "source": "application", 
            "format": "bracketed"
        }
    
    def parse_web_log(self, match, original_line):
        """ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ð²ÐµÐ±-Ð»Ð¾Ð³Ð¸"""
        return {
            "message": original_line.strip(),
            "level": "INFO",  # web logs Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾ INFO
            "timestamp": match.group(2),
            "source": "web_server",
            "client_ip": match.group(1),
            "format": "web"
        }
    
    def fallback_parse(self, line):
        """Fallback Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Ð´Ð»Ñ Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ñ… Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¾Ð²"""
        return {
            "message": line.strip(),
            "level": self.detect_log_level(line),
            "timestamp": datetime.now().isoformat(), 
            "source": "unknown",
            "format": "plain"
        }
    
    def detect_log_level(self, message):
        """ÐÐ²Ñ‚Ð¾-Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ ÑƒÑ€Ð¾Ð²Ð½Ñ Ð»Ð¾Ð³Ð°"""
        message_lower = message.lower()
        
        error_keywords = ['error', 'exception', 'failed', 'fatal', 'crash', 'panic']
        warn_keywords = ['warn', 'warning', 'deprecated', 'slow', 'timeout']
        debug_keywords = ['debug', 'trace', 'verbose']
        
        if any(word in message_lower for word in error_keywords):
            return "ERROR"
        elif any(word in message_lower for word in warn_keywords):
            return "WARN" 
        elif any(word in message_lower for word in debug_keywords):
            return "DEBUG"
        else:
            return "INFO"
    
    def process_line(self, line):
        """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¾Ð´Ð½Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð¸Ð· stdin"""
        if not line.strip():
            return
            
        try:
            # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
            log_data = self.extract_log_metadata(line)
            self.batch_buffer.append(log_data)
            
            # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ñ‚Ð°Ð¹Ð¼ÐµÑ€
            self.reset_timer()
            
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð±Ð°Ñ‚Ñ‡Ð°
            if len(self.batch_buffer) >= self.batch_size:
                self.flush_batch()
                
            self.processed_count += 1
            
            # ÐŸÐµÑ€Ð¸Ð¾Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ ÑÑ‚Ð°Ñ‚ÑƒÑ
            if self.processed_count % 100 == 0:
                elapsed = time.time() - self.start_time
                rate = self.processed_count / elapsed
                print(f"ðŸ“Š Processed {self.processed_count} logs ({rate:.1f}/sec)", 
                      file=sys.stderr)
                      
        except Exception as e:
            print(f"âŒ Error processing line: {e}", file=sys.stderr)
    
    def flush_batch(self):
        """ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° Ð½Ð°ÐºÐ¾Ð¿Ð»ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð±Ð°Ñ‚Ñ‡Ð° Ð² Qdrant"""
        if not self.batch_buffer:
            return
            
        try:
            # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ð´Ð»Ñ Ð²ÑÐµÑ… ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð² Ð±Ð°Ñ‚Ñ‡Ðµ
            messages = [log["message"] for log in self.batch_buffer]
            embeddings = self.model.encode(messages)
            
            # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ñ‚Ð¾Ñ‡ÐºÐ¸ Ð´Ð»Ñ Qdrant
            points = []
            for i, (log, embedding) in enumerate(zip(self.batch_buffer, embeddings)):
                point_id = int(time.time() * 1000000) + i  # microsecond precision
                
                points.append(models.PointStruct(
                    id=point_id,
                    vector=embedding.tolist(),
                    payload={
                        "message": log["message"],
                        "level": log["level"],
                        "timestamp": log["timestamp"],
                        "source": log["source"],
                        "format": log["format"],
                        "processed_at": datetime.now().isoformat(),
                        "batch_size": len(self.batch_buffer)
                    }
                ))
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð² Qdrant
            self.client.upsert(
                collection_name=self.collection_name,
                points=points
            )
            
            print(f"âœ… Saved {len(points)} logs to {self.collection_name}", 
                  file=sys.stderr)
            
        except Exception as e:
            print(f"âŒ Batch flush error: {e}", file=sys.stderr)
        finally:
            self.batch_buffer.clear()
    
    def run(self):
        """ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ñ†Ð¸ÐºÐ» Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ stdin"""
        print(f"ðŸš€ Universal Log Processor started", file=sys.stderr)
        print(f"ðŸ“ Collection: {self.collection_name}", file=sys.stderr)
        print(f"âš™ï¸  Batch: {self.batch_size} logs or {self.batch_timeout}s", file=sys.stderr)
        print("---", file=sys.stderr)
        
        try:
            for line in sys.stdin:
                self.process_line(line)
                
        except KeyboardInterrupt:
            print("\nðŸ›‘ Shutdown signal received...", file=sys.stderr)
        except BrokenPipeError:
            print("ðŸ’¥ Input pipe broken", file=sys.stderr)
        except Exception as e:
            print(f"ðŸ’¥ Fatal error: {e}", file=sys.stderr)
        finally:
            # Ð“Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾ ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¾ÑÑ‚Ð°Ð²ÑˆÐ¸ÐµÑÑ Ð»Ð¾Ð³Ð¸
            if self.flush_timer:
                self.flush_timer.cancel()
            if self.batch_buffer:
                print(f"ðŸ’¾ Flushing {len(self.batch_buffer)} remaining logs...", 
                      file=sys.stderr)
                self.flush_batch()
            
            elapsed = time.time() - self.start_time
            print(f"ðŸ‘‹ Processor stopped. Stats:", file=sys.stderr)
            print(f"   Total processed: {self.processed_count} logs", file=sys.stderr)
            print(f"   Duration: {elapsed:.1f} seconds", file=sys.stderr)
            print(f"   Rate: {self.processed_count/elapsed:.1f} logs/sec", file=sys.stderr)

if __name__ == "__main__":
    # ÐœÐ¾Ð¶Ð½Ð¾ ÑƒÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð¸Ð¼Ñ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸ Ñ‡ÐµÑ€ÐµÐ· Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚
    collection_name = sys.argv[1] if len(sys.argv) > 1 else "universal-logs"
    processor = UniversalLogProcessor(collection_name)
    processor.run()
